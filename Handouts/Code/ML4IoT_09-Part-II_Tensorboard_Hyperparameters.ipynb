{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard HyperParameters Logging\n",
    "\n",
    "Source: https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    "\n",
    "In this notebook, we'll use TensorBoard to compare multiple training runs with different hyper-parameters settings. Let us start by adding the TB extension to jupyter and clearing the logs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./tb_log/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the required modules. We'll use the `hparams` plugin of Keras to explore hyper-parameters settings. This plugin has several other functionalities not touched in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset\n",
    "\n",
    "We'll use again Fashion MNIST for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model\n",
    "\n",
    "We'll use a function to build a different version of the model based on some hyperparameters. In particular, we'll use only two hyperparameters here for simplicity:\n",
    "- The number of units in the first fully-connected layer of our NN (`hparams['num_units']`)\n",
    "- The dropout rate of that same layer (`hparams['dropout']`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_hp_setting(hparams):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(hparams['num_units'], activation=tf.nn.relu),\n",
    "        keras.layers.Dropout(hparams['dropout']),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model\n",
    "\n",
    "We'll train the model multiple times for different settings of the two hyper-parameters. Each run will be identified by an index (`session_num`). The corresponding logs will be saved in a separate sub-folder for TensorBoard.\n",
    "\n",
    "The Keras `hparams` plugin has a dedicated callback (`hp.KerasCallback`) which logs the values of the hyper-parameters passed as input associated with each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num_units in [512, 1024]:\n",
    "for num_units in [16, 64]:\n",
    "  for dropout_rate in [0.1, 0.5]:\n",
    "        hparams = {'num_units': num_units, 'dropout': dropout_rate}\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print(hparams)\n",
    "    \n",
    "        log_dir = 'tb_log/hparam_tuning/' + run_name\n",
    "        tb_callback = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "        hp_callback = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "        model = try_hp_setting(hparams)\n",
    "        model.fit(x_train, y_train, validation_split=0.1, epochs=10, callbacks=[tb_callback, hp_callback])\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Tensorboard\n",
    "\n",
    "Let's start tensorboard and see the results of this experiment. We can clearly inspect the scalar logs as seen in the previous notebook. However, the `HPARAMS` tab provides a more useful visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './tb_log'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
